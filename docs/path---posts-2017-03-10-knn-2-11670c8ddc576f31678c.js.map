{"version":3,"sources":["webpack:///path---posts-2017-03-10-knn-2-11670c8ddc576f31678c.js","webpack:///./.cache/json/posts-2017-03-10-knn-2.json"],"names":["webpackJsonp","438","module","exports","data","markdownRemark","html","frontmatter","title","date","tags","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,olFAAqgEC,aAAqoBC,MAAA,mBAAAC,KAAA,aAAAC,MAAA,eAAqEC,aAAgBC,KAAA","file":"path---posts-2017-03-10-knn-2-11670c8ddc576f31678c.js","sourcesContent":["webpackJsonp([198895678656149],{\n\n/***/ 438:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h1>机器学习之k-NN算法实战(二)</h1>\\n<p>回顾上文，我们介绍了k-NN算法的原理，以及实例数据的格式，如何加载。万事俱备，下文我们来介绍k-NN算法的代码实现。</p>\\n<p>首先从store中取出样本数据，存到局部变量中</p>\\n<pre><code class=\\\"language-py\\\">data_store = parse_data()\\n# 测试样本向量\\ntest_vectors = data_store[\\\"test_set\\\"][\\\"vectors\\\"]\\n# 测试样本的标签\\ntest_labels = data_store[\\\"test_set\\\"][\\\"labels\\\"]\\n# 训练样本的标签\\ntrain_vectors_labels = data_store[\\\"train_set\\\"][\\\"labels\\\"]\\n# 训练样本向量\\ntrain_vectors_mat = data_store[\\\"train_set\\\"][\\\"vectors\\\"]\\n</code></pre>\\n<p>之后我们利用这些数据来进行k-NN分类</p>\\n<pre><code class=\\\"language-py\\\">def classify(test_data_vector, training_data_mat, k):\\n    # 复制向量成矩阵\\n    test_data_mat = np.tile(test_data_vector, (training_data_mat.shape[0], 1))\\n    # 两个矩阵求内部向量的欧氏距离\\n    diff = test_data_mat - training_data_mat\\n    euclidean_dis = ((diff**2).sum(axis=1)) ** 0.5\\n    # 排序\\n    index_sorted = euclidean_dis.argsort()\\n    return index_sorted[:k]\\n</code></pre>\\n<p>上面的函数，我们需要的参数为测试向量，训练向量集，以及K值。我们将待测试的向量铺成和训练集相同shape的矩阵，之后直接求欧氏距离，按照预设去k个离被测试向量最近的训练向量，排序。</p>\\n<p>除了进行排序，我们还需要测试错误数据的占比，得出结论，这样才可以判断算法的优劣。</p>\\n<pre><code class=\\\"language-py\\\">def cal_vote(nearest_index, label, train_vectors_labels):\\n    # 统计错误的票数\\n    error_count = 0.0\\n    for i in range(len(nearest_index)):\\n        index = nearest_index[i]\\n        if train_vectors_labels[index] != label:\\n            error_count += 1\\n    # 错误数量过半时\\n    if error_count >= math.ceil(len(nearest_index)/2):\\n        return True\\n    else:\\n        return False\\n</code></pre>\\n<p>cal<em>vote函数接受三个参数，nearest</em>index是距离待测点最近的几个向量的索引，正确label，测试的label。之后计算投票，这里按照少数服从多数，有时候还可以按照距离的权重为各个向量加权。\\n之后我们调用上文的函数，代码如下</p>\\n<pre><code class=\\\"language-py\\\"># 分类错误的总数\\nsum_error_count = 0.0\\nfor i in range(len(test_vectors)):\\n    nearest_index = classify(test_vectors[i], train_vectors_mat, K)\\n    result = cal_vote(nearest_index, test_labels[i], train_vectors_labels)\\n    if not result:\\n        sum_error_count += 1\\n# 输出信息\\nprint(\\\"\\\\n当前位 k-NN 算法,k值为：{0}\\\".format(K))\\nprint(\\\"\\\\n分类结束，分类器分类总错误数为：{0}\\\".format(sum_error_count))\\nprint(\\\"\\\\n分类器分类错误率为：{0}%\\\".format((sum_error_count / len(train_vectors_labels))*100))\\n# 这里的k值设置为20，k值不同经常会影响分类器的效果，k=20时分类器在当前样本上，效果还不错\\n</code></pre>\\n<p>以上是算法部分，代码文件在<a href=\\\"https://github.com/getcha22/Machine-Learning/blob/master/k-NN/k-NN.py\\\">GITHUB</a>仓库中也存有备份,大家可以直接查看。</p>\\n<h2>总结</h2>\\n<p>k-NN算法的原理和实战代码部分见上文，接着我们来看看这个算法的优缺点，试用条件。</p>\\n<p>优点：</p>\\n<ul>\\n<li>简单明了，粗暴快速。这个是肯定的，核心原理就是计算两个向量的某种距离。</li>\\n<li>准确率尚可，对于一些基础的分类任务，它非常适合</li>\\n</ul>\\n<p>缺点：</p>\\n<ul>\\n<li>性能较弱，k-NN算法是懒惰学习方法，它是根据所给训练样本构造的分类器，是将所有训练样本首先存储起来，当要进行分类时，就直接进行计算处理。时间复杂度和空间复杂度都较高。</li>\\n<li>K值难以有一个确值，需要不断试。</li>\\n<li>对于非数值型的样本以及离散型的样本，我们需要对其进行数值化和连续化，比如红黄蓝等等。</li>\\n</ul>\",\"frontmatter\":{\"title\":\"机器学习之k-NN算法实战(二)\",\"date\":\"2017-03-10\",\"tags\":[\"ML\",\"KNN\"]}}},\"pathContext\":{\"slug\":\"/2017-03-10---knn-2/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---posts-2017-03-10-knn-2-11670c8ddc576f31678c.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h1>机器学习之k-NN算法实战(二)</h1>\\n<p>回顾上文，我们介绍了k-NN算法的原理，以及实例数据的格式，如何加载。万事俱备，下文我们来介绍k-NN算法的代码实现。</p>\\n<p>首先从store中取出样本数据，存到局部变量中</p>\\n<pre><code class=\\\"language-py\\\">data_store = parse_data()\\n# 测试样本向量\\ntest_vectors = data_store[\\\"test_set\\\"][\\\"vectors\\\"]\\n# 测试样本的标签\\ntest_labels = data_store[\\\"test_set\\\"][\\\"labels\\\"]\\n# 训练样本的标签\\ntrain_vectors_labels = data_store[\\\"train_set\\\"][\\\"labels\\\"]\\n# 训练样本向量\\ntrain_vectors_mat = data_store[\\\"train_set\\\"][\\\"vectors\\\"]\\n</code></pre>\\n<p>之后我们利用这些数据来进行k-NN分类</p>\\n<pre><code class=\\\"language-py\\\">def classify(test_data_vector, training_data_mat, k):\\n    # 复制向量成矩阵\\n    test_data_mat = np.tile(test_data_vector, (training_data_mat.shape[0], 1))\\n    # 两个矩阵求内部向量的欧氏距离\\n    diff = test_data_mat - training_data_mat\\n    euclidean_dis = ((diff**2).sum(axis=1)) ** 0.5\\n    # 排序\\n    index_sorted = euclidean_dis.argsort()\\n    return index_sorted[:k]\\n</code></pre>\\n<p>上面的函数，我们需要的参数为测试向量，训练向量集，以及K值。我们将待测试的向量铺成和训练集相同shape的矩阵，之后直接求欧氏距离，按照预设去k个离被测试向量最近的训练向量，排序。</p>\\n<p>除了进行排序，我们还需要测试错误数据的占比，得出结论，这样才可以判断算法的优劣。</p>\\n<pre><code class=\\\"language-py\\\">def cal_vote(nearest_index, label, train_vectors_labels):\\n    # 统计错误的票数\\n    error_count = 0.0\\n    for i in range(len(nearest_index)):\\n        index = nearest_index[i]\\n        if train_vectors_labels[index] != label:\\n            error_count += 1\\n    # 错误数量过半时\\n    if error_count >= math.ceil(len(nearest_index)/2):\\n        return True\\n    else:\\n        return False\\n</code></pre>\\n<p>cal<em>vote函数接受三个参数，nearest</em>index是距离待测点最近的几个向量的索引，正确label，测试的label。之后计算投票，这里按照少数服从多数，有时候还可以按照距离的权重为各个向量加权。\\n之后我们调用上文的函数，代码如下</p>\\n<pre><code class=\\\"language-py\\\"># 分类错误的总数\\nsum_error_count = 0.0\\nfor i in range(len(test_vectors)):\\n    nearest_index = classify(test_vectors[i], train_vectors_mat, K)\\n    result = cal_vote(nearest_index, test_labels[i], train_vectors_labels)\\n    if not result:\\n        sum_error_count += 1\\n# 输出信息\\nprint(\\\"\\\\n当前位 k-NN 算法,k值为：{0}\\\".format(K))\\nprint(\\\"\\\\n分类结束，分类器分类总错误数为：{0}\\\".format(sum_error_count))\\nprint(\\\"\\\\n分类器分类错误率为：{0}%\\\".format((sum_error_count / len(train_vectors_labels))*100))\\n# 这里的k值设置为20，k值不同经常会影响分类器的效果，k=20时分类器在当前样本上，效果还不错\\n</code></pre>\\n<p>以上是算法部分，代码文件在<a href=\\\"https://github.com/getcha22/Machine-Learning/blob/master/k-NN/k-NN.py\\\">GITHUB</a>仓库中也存有备份,大家可以直接查看。</p>\\n<h2>总结</h2>\\n<p>k-NN算法的原理和实战代码部分见上文，接着我们来看看这个算法的优缺点，试用条件。</p>\\n<p>优点：</p>\\n<ul>\\n<li>简单明了，粗暴快速。这个是肯定的，核心原理就是计算两个向量的某种距离。</li>\\n<li>准确率尚可，对于一些基础的分类任务，它非常适合</li>\\n</ul>\\n<p>缺点：</p>\\n<ul>\\n<li>性能较弱，k-NN算法是懒惰学习方法，它是根据所给训练样本构造的分类器，是将所有训练样本首先存储起来，当要进行分类时，就直接进行计算处理。时间复杂度和空间复杂度都较高。</li>\\n<li>K值难以有一个确值，需要不断试。</li>\\n<li>对于非数值型的样本以及离散型的样本，我们需要对其进行数值化和连续化，比如红黄蓝等等。</li>\\n</ul>\",\"frontmatter\":{\"title\":\"机器学习之k-NN算法实战(二)\",\"date\":\"2017-03-10\",\"tags\":[\"ML\",\"KNN\"]}}},\"pathContext\":{\"slug\":\"/2017-03-10---knn-2/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/posts-2017-03-10-knn-2.json\n// module id = 438\n// module chunks = 198895678656149"],"sourceRoot":""}